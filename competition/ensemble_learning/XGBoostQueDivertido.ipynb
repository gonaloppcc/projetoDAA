{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação do Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from pandas import DataFrame\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "TRAINING_DATASET_SOURCE = '../datasets/training_data.csv'  # Since we are one directory up, we should go down one directory to import the datasets\n",
    "TEST_DATASET_SOURCE = '../datasets/test_data.csv'\n",
    "\n",
    "train_df = pd.read_csv(TRAINING_DATASET_SOURCE)\n",
    "test_df = pd.read_csv(TEST_DATASET_SOURCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição dos dados de teste e de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 13) (1206, 12)\n",
      "None         2028\n",
      "High         1073\n",
      "Low           718\n",
      "Very_High     603\n",
      "Medium        578\n",
      "Name: incidents, dtype: int64\n",
      "Max value count: 2028\n",
      "None         2028\n",
      "High         2028\n",
      "Low          2028\n",
      "Very_High    2028\n",
      "Medium       2028\n",
      "Name: incidents, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape, test_df.shape)\n",
    "print(train_df['incidents'].value_counts())\n",
    "\n",
    "incidents_count = train_df['incidents'].value_counts()\n",
    "\n",
    "max_count = incidents_count.max()\n",
    "print('Max value count:', max_count)\n",
    "\n",
    "df_classes = []\n",
    "for category, counts in zip(incidents_count.index, incidents_count):\n",
    "    #print(category, counts)\n",
    "    df_classes.append(train_df[train_df['incidents'] == category])\n",
    "\n",
    "df_classes_over = []\n",
    "\n",
    "for category in df_classes:\n",
    "    df_classes_over.append(category.sample(max_count, replace=True))\n",
    "\n",
    "df_test_over = pd.concat(df_classes_over, axis=0)\n",
    "\n",
    "print(df_test_over['incidents'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>city_name</th>\n",
       "      <th>magnitude_of_delay</th>\n",
       "      <th>delay_in_seconds</th>\n",
       "      <th>affected_roads</th>\n",
       "      <th>record_date</th>\n",
       "      <th>luminosity</th>\n",
       "      <th>avg_temperature</th>\n",
       "      <th>avg_atm_pressure</th>\n",
       "      <th>avg_humidity</th>\n",
       "      <th>avg_wind_speed</th>\n",
       "      <th>avg_precipitation</th>\n",
       "      <th>avg_rain</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>Guimaraes</td>\n",
       "      <td>UNDEFINED</td>\n",
       "      <td>0</td>\n",
       "      <td>N101,N101,N101,N101,N101</td>\n",
       "      <td>2021-07-27 09:00</td>\n",
       "      <td>LIGHT</td>\n",
       "      <td>18.0</td>\n",
       "      <td>1016.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sem Chuva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3620</th>\n",
       "      <td>Guimaraes</td>\n",
       "      <td>UNDEFINED</td>\n",
       "      <td>0</td>\n",
       "      <td>N101,N101,N101,N101,N101</td>\n",
       "      <td>2021-10-09 14:00</td>\n",
       "      <td>LIGHT</td>\n",
       "      <td>24.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sem Chuva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>470</th>\n",
       "      <td>Guimaraes</td>\n",
       "      <td>UNDEFINED</td>\n",
       "      <td>0</td>\n",
       "      <td>,</td>\n",
       "      <td>2021-12-27 08:00</td>\n",
       "      <td>LOW_LIGHT</td>\n",
       "      <td>12.0</td>\n",
       "      <td>1014.0</td>\n",
       "      <td>94.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sem Chuva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2645</th>\n",
       "      <td>Guimaraes</td>\n",
       "      <td>UNDEFINED</td>\n",
       "      <td>0</td>\n",
       "      <td>N101,N101,N101</td>\n",
       "      <td>2021-12-28 22:00</td>\n",
       "      <td>DARK</td>\n",
       "      <td>13.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sem Chuva</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>402</th>\n",
       "      <td>Guimaraes</td>\n",
       "      <td>UNDEFINED</td>\n",
       "      <td>0</td>\n",
       "      <td>N101,N101,N101,N101,N101</td>\n",
       "      <td>2021-11-12 01:00</td>\n",
       "      <td>DARK</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1019.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Sem Chuva</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      city_name magnitude_of_delay  delay_in_seconds  \\\n",
       "804   Guimaraes          UNDEFINED                 0   \n",
       "3620  Guimaraes          UNDEFINED                 0   \n",
       "470   Guimaraes          UNDEFINED                 0   \n",
       "2645  Guimaraes          UNDEFINED                 0   \n",
       "402   Guimaraes          UNDEFINED                 0   \n",
       "\n",
       "                affected_roads       record_date luminosity  avg_temperature  \\\n",
       "804   N101,N101,N101,N101,N101  2021-07-27 09:00      LIGHT             18.0   \n",
       "3620  N101,N101,N101,N101,N101  2021-10-09 14:00      LIGHT             24.0   \n",
       "470                          ,  2021-12-27 08:00  LOW_LIGHT             12.0   \n",
       "2645            N101,N101,N101  2021-12-28 22:00       DARK             13.0   \n",
       "402   N101,N101,N101,N101,N101  2021-11-12 01:00       DARK             10.0   \n",
       "\n",
       "      avg_atm_pressure  avg_humidity  avg_wind_speed  avg_precipitation  \\\n",
       "804             1016.0          85.0             0.0                0.0   \n",
       "3620            1019.0          65.0             0.0                0.0   \n",
       "470             1014.0          94.0             2.0                0.0   \n",
       "2645            1024.0          93.0             1.0                0.0   \n",
       "402             1019.0          93.0             1.0                0.0   \n",
       "\n",
       "       avg_rain  \n",
       "804   Sem Chuva  \n",
       "3620  Sem Chuva  \n",
       "470   Sem Chuva  \n",
       "2645  Sem Chuva  \n",
       "402   Sem Chuva  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df_test_over.drop(['incidents'], axis=1)\n",
    "target = df_test_over['incidents']\n",
    "\n",
    "all_features = features.columns.tolist()\n",
    "\n",
    "features[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenção das features numericas e categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "categorical_to_numerical = {\n",
    "    'luminosity': {\n",
    "        'LOW_LIGHT': 0,\n",
    "        'LIGHT': 1,\n",
    "        'DARK': 2\n",
    "    },\n",
    "    'avg_rain': {\n",
    "        'Sem Chuva': 0,\n",
    "        'chuva fraca': 1,\n",
    "        'chuva moderada': 2,\n",
    "        'chuva forte': 3,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def decision_tree_data_preparation(df: DataFrame) -> DataFrame:\n",
    "    prep_df = df.copy()\n",
    "\n",
    "    dropped_columns = ['city_name', 'avg_precipitation', 'magnitude_of_delay']\n",
    "\n",
    "    numerical_features = [column for column, dtype in zip(features.columns, features.dtypes) if\n",
    "                          dtype.kind in ['i', 'f'] and column not in dropped_columns]\n",
    "\n",
    "    numerical_features = ['avg_temperature', 'avg_atm_pressure', 'avg_humidity', 'avg_wind_speed', 'luminosity']\n",
    "    #assert numerical_features == ['avg_temperature', 'avg_atm_pressure', 'avg_humidity', 'avg_wind_speed', 'luminosity']\n",
    "\n",
    "    categorical_features = [column for column, dtype in zip(features.columns, features.dtypes) if\n",
    "                            dtype.kind not in ['i', 'f'] and column not in dropped_columns]\n",
    "\n",
    "    prep_df = prep_df.drop(dropped_columns, axis=1)\n",
    "    prep_df.drop_duplicates()\n",
    "\n",
    "    ### Converter as features categoricas em numericas\n",
    "    prep_df.replace(categorical_to_numerical, inplace=True)\n",
    "\n",
    "    ### Extrair a hora e dia da semana da feature 'record_date'\n",
    "    record_date = pd.DatetimeIndex(prep_df['record_date'])\n",
    "\n",
    "    prep_df['hour'] = record_date.hour\n",
    "    prep_df['day'] = record_date.day\n",
    "    prep_df['month'] = record_date.month\n",
    "    prep_df['weekday'] = record_date.weekday\n",
    "\n",
    "    prep_df.drop(columns=['record_date'], inplace=True)\n",
    "\n",
    "    #train_df['affected_roads'] = train_df['affected_roads'].fillna(train_df['affected_roads'].mode().iloc[0])\n",
    "\n",
    "    num_affected_roads = []\n",
    "    for line in prep_df['affected_roads']:\n",
    "        unique_roads = set(str(line).split(','))\n",
    "        valid_roads = [elem for elem in unique_roads if elem != '']\n",
    "        count = len(valid_roads)\n",
    "        num_affected_roads.append(count)\n",
    "\n",
    "    prep_df['num_affected_roads'] = num_affected_roads\n",
    "\n",
    "    ### Ao analisar o resultado pós-tratamento, verificámos que a feature 'affected_roads' tinha alta correlação com 'delay_in_minutes'\n",
    "    prep_df.drop(columns=['affected_roads'], inplace=True)\n",
    "\n",
    "    ### Converter a feature 'delay_in_seconds' para 'delay_in_minutes' de modo a reduzir o intervalo de valores\n",
    "    delay_in_minutes = prep_df['delay_in_seconds'].map(lambda seconds: seconds / 60)\n",
    "\n",
    "    prep_df.drop(columns=['delay_in_seconds'], inplace=True)\n",
    "    prep_df['delay_in_minutes'] = delay_in_minutes\n",
    "\n",
    "    ### Limites superior e inferior (sem outliers) dos diagramas de caixa\n",
    "    #numerical_features.remove('delay_in_seconds')\n",
    "    #numerical_features.append('delay_in_minutes')\n",
    "\n",
    "    return prep_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão dos dados em dados de teste e treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>incidents</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10135</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10136</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10137</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10138</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10139</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10140 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       incidents\n",
       "0              0\n",
       "1              0\n",
       "2              0\n",
       "3              0\n",
       "4              0\n",
       "...          ...\n",
       "10135          2\n",
       "10136          2\n",
       "10137          2\n",
       "10138          2\n",
       "10139          2\n",
       "\n",
       "[10140 rows x 1 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_num = pd.DataFrame(target.map({'None': 0, 'Low': 1, 'Medium': 2, 'High': 3, 'Very_High': 4}))\n",
    "target_num.reset_index(drop=True, inplace=True)\n",
    "target_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(decision_tree_data_preparation(features), target_num, test_size=0.3,\n",
    "                                                    random_state=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      incidents\n",
      "8721          2\n",
      "5465          1\n",
      "2527          3\n",
      "5321          1\n",
      "3987          3\n",
      "...         ...\n",
      "9628          2\n",
      "4380          1\n",
      "1590          0\n",
      "4045          3\n",
      "4936          1\n",
      "\n",
      "[7098 rows x 1 columns]\n",
      "      incidents\n",
      "1564          0\n",
      "9052          2\n",
      "8684          2\n",
      "7491          4\n",
      "389           0\n",
      "...         ...\n",
      "1288          0\n",
      "4606          1\n",
      "4999          1\n",
      "6644          4\n",
      "2501          3\n",
      "\n",
      "[3042 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "#boost_model = XGBClassifier(n_estimators=500, early_stopping_rounds=5, random_state=22)\n",
    "#boost_model.fit(X_train, y_train, eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#print(\"Train accuracy:\", boost_model.score(X_train, y_train))\n",
    "\n",
    "#predictions = boost_model.predict(X_test)\n",
    "#print(\"Test accuracy: \" + str(accuracy_score(predictions, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gonca\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\gonca\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:06:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'None'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 37\u001b[0m\n\u001b[0;32m     34\u001b[0m clf\u001b[38;5;241m.\u001b[39mfit(X_train, y_train)\n\u001b[0;32m     36\u001b[0m estimators\u001b[38;5;241m.\u001b[39mappend(clf\u001b[38;5;241m.\u001b[39mbest_estimator_)\n\u001b[1;32m---> 37\u001b[0m \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtest_index\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     38\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m accuracy_score(results[test_index], y_test)\n\u001b[0;32m     39\u001b[0m lista_resultados\u001b[38;5;241m.\u001b[39mappend(accuracy_score(results[test_index], y_test))\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'None'"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X = decision_tree_data_preparation(features)\n",
    "y = pd.DataFrame(target)\n",
    "print(type(y_test))\n",
    "#y = pd.concat([y_test, y_train])\n",
    "clf_xgb = XGBClassifier(objective='binary:logistic')\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': stats.randint(150, 500),\n",
    "    'learning_rate': stats.uniform(0.01, 0.07),\n",
    "    'subsample': stats.uniform(0.3, 0.7),\n",
    "    'max_depth': [3, 5, 8],\n",
    "    'colsample_bytree': stats.uniform(0.5, 0.45),\n",
    "    'min_child_weight': [1, 3]\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(clf_xgb, param_distributions=param_dist, verbose=3,\n",
    "                         cv=2, n_iter=5, scoring='accuracy', error_score=0,\n",
    "                         n_jobs=-1)\n",
    "numFolds = 2\n",
    "folds = KFold(n_splits=numFolds, shuffle=True)\n",
    "\n",
    "estimators = []\n",
    "results = np.zeros(len(X))\n",
    "score = 0.0\n",
    "lista_resultados = []\n",
    "for train_index, test_index in folds.split(X):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y.iloc[train_index].values.ravel(), y.iloc[test_index].values.ravel()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    estimators.append(clf.best_estimator_)\n",
    "    results[test_index] = clf.predict(X_test)\n",
    "    score += accuracy_score(results[test_index], y_test)\n",
    "    lista_resultados.append(accuracy_score(results[test_index], y_test))\n",
    "    #score += f1_score(y_test, results[test_index])\n",
    "score /= numFolds\n",
    "\n",
    "print(f\"Minimo é: {min(lista_resultados)}\")\n",
    "'''\n",
    "def hyper_param(model, params):\n",
    "    print(\"Modelo atual: \" , model)\n",
    "    \n",
    "    # cv = cross-validation generator\n",
    "    # verbose = quanto é apresentado\n",
    "    tuning_model=GridSearchCV(model,param_grid=params,scoring='neg_mean_squared_error',cv=3)\n",
    "    tuning_model.fit(X_train, y_train)\n",
    "    \n",
    "    return tuning_model.best_params_\n",
    "\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:squarederror',\n",
    "}\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=999,\n",
    "    seed=42,\n",
    "    nfold=5,\n",
    "    metrics = 'error',\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "print(cv_results)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenção das previsões do dataset de submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_df.copy()\n",
    "\n",
    "test_data_prepared = decision_tree_data_preparation(test_data)\n",
    "\n",
    "predictions = boost_model.predict(test_data_prepared)  #RF_Model.predict(test_data_prepared)\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df = predictions_df[0].map(\n",
    "    {0: 'None', 1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very_High'})\n",
    "predictions_df.index += 1\n",
    "predictions_df.to_csv(\"../submission_v2.csv\", header=['Incidents'], index_label='RowId')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1 (main, Dec 23 2022, 09:25:23) [Clang 14.0.0 (clang-1400.0.29.202)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "5c7b89af1651d0b8571dde13640ecdccf7d5a6204171d6ab33e7c296e100e08a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
