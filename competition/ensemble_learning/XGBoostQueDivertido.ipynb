{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementação do Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder, LabelEncoder\n",
    "from pandas import DataFrame\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "TRAINING_DATASET_SOURCE = '../training_data.csv'  # Since we are one directory up, we should go down one directory to import the datasets\n",
    "TEST_DATASET_SOURCE = '../test_data.csv'\n",
    "\n",
    "train_df = pd.read_csv(TRAINING_DATASET_SOURCE)\n",
    "test_df = pd.read_csv(TEST_DATASET_SOURCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definição dos dados de teste e de treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 13) (1206, 12)\n",
      "None         2028\n",
      "High         1073\n",
      "Low           718\n",
      "Very_High     603\n",
      "Medium        578\n",
      "Name: incidents, dtype: int64\n",
      "Max value count: 2028\n",
      "None         2028\n",
      "High         2028\n",
      "Low          2028\n",
      "Very_High    2028\n",
      "Medium       2028\n",
      "Name: incidents, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_df.shape, test_df.shape)\n",
    "print(train_df['incidents'].value_counts())\n",
    "\n",
    "incidents_count = train_df['incidents'].value_counts()\n",
    "\n",
    "max_count = incidents_count.max()\n",
    "print('Max value count:', max_count)\n",
    "\n",
    "df_classes = []\n",
    "for category, counts in zip(incidents_count.index, incidents_count):\n",
    "    #print(category, counts)\n",
    "    df_classes.append(train_df[train_df['incidents'] == category])\n",
    "\n",
    "df_classes_over = []\n",
    "\n",
    "for category in df_classes:\n",
    "    df_classes_over.append(category.sample(max_count, replace=True))\n",
    "\n",
    "df_test_over = pd.concat(df_classes_over, axis=0)\n",
    "\n",
    "print(df_test_over['incidents'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "      city_name magnitude_of_delay  delay_in_seconds  \\\n804   Guimaraes          UNDEFINED                 0   \n3620  Guimaraes          UNDEFINED                 0   \n470   Guimaraes          UNDEFINED                 0   \n2645  Guimaraes          UNDEFINED                 0   \n402   Guimaraes          UNDEFINED                 0   \n\n                affected_roads       record_date luminosity  avg_temperature  \\\n804   N101,N101,N101,N101,N101  2021-07-27 09:00      LIGHT             18.0   \n3620  N101,N101,N101,N101,N101  2021-10-09 14:00      LIGHT             24.0   \n470                          ,  2021-12-27 08:00  LOW_LIGHT             12.0   \n2645            N101,N101,N101  2021-12-28 22:00       DARK             13.0   \n402   N101,N101,N101,N101,N101  2021-11-12 01:00       DARK             10.0   \n\n      avg_atm_pressure  avg_humidity  avg_wind_speed  avg_precipitation  \\\n804             1016.0          85.0             0.0                0.0   \n3620            1019.0          65.0             0.0                0.0   \n470             1014.0          94.0             2.0                0.0   \n2645            1024.0          93.0             1.0                0.0   \n402             1019.0          93.0             1.0                0.0   \n\n       avg_rain  \n804   Sem Chuva  \n3620  Sem Chuva  \n470   Sem Chuva  \n2645  Sem Chuva  \n402   Sem Chuva  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>city_name</th>\n      <th>magnitude_of_delay</th>\n      <th>delay_in_seconds</th>\n      <th>affected_roads</th>\n      <th>record_date</th>\n      <th>luminosity</th>\n      <th>avg_temperature</th>\n      <th>avg_atm_pressure</th>\n      <th>avg_humidity</th>\n      <th>avg_wind_speed</th>\n      <th>avg_precipitation</th>\n      <th>avg_rain</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>804</th>\n      <td>Guimaraes</td>\n      <td>UNDEFINED</td>\n      <td>0</td>\n      <td>N101,N101,N101,N101,N101</td>\n      <td>2021-07-27 09:00</td>\n      <td>LIGHT</td>\n      <td>18.0</td>\n      <td>1016.0</td>\n      <td>85.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Sem Chuva</td>\n    </tr>\n    <tr>\n      <th>3620</th>\n      <td>Guimaraes</td>\n      <td>UNDEFINED</td>\n      <td>0</td>\n      <td>N101,N101,N101,N101,N101</td>\n      <td>2021-10-09 14:00</td>\n      <td>LIGHT</td>\n      <td>24.0</td>\n      <td>1019.0</td>\n      <td>65.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>Sem Chuva</td>\n    </tr>\n    <tr>\n      <th>470</th>\n      <td>Guimaraes</td>\n      <td>UNDEFINED</td>\n      <td>0</td>\n      <td>,</td>\n      <td>2021-12-27 08:00</td>\n      <td>LOW_LIGHT</td>\n      <td>12.0</td>\n      <td>1014.0</td>\n      <td>94.0</td>\n      <td>2.0</td>\n      <td>0.0</td>\n      <td>Sem Chuva</td>\n    </tr>\n    <tr>\n      <th>2645</th>\n      <td>Guimaraes</td>\n      <td>UNDEFINED</td>\n      <td>0</td>\n      <td>N101,N101,N101</td>\n      <td>2021-12-28 22:00</td>\n      <td>DARK</td>\n      <td>13.0</td>\n      <td>1024.0</td>\n      <td>93.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>Sem Chuva</td>\n    </tr>\n    <tr>\n      <th>402</th>\n      <td>Guimaraes</td>\n      <td>UNDEFINED</td>\n      <td>0</td>\n      <td>N101,N101,N101,N101,N101</td>\n      <td>2021-11-12 01:00</td>\n      <td>DARK</td>\n      <td>10.0</td>\n      <td>1019.0</td>\n      <td>93.0</td>\n      <td>1.0</td>\n      <td>0.0</td>\n      <td>Sem Chuva</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = df_test_over.drop(['incidents'], axis=1)\n",
    "target = df_test_over['incidents']\n",
    "\n",
    "all_features = features.columns.tolist()\n",
    "\n",
    "features[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenção das features numericas e categoricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "categorical_to_numerical = {\n",
    "    'luminosity': {\n",
    "        'LOW_LIGHT': 0,\n",
    "        'LIGHT': 1,\n",
    "        'DARK': 2\n",
    "    },\n",
    "    'avg_rain': {\n",
    "        'Sem Chuva': 0,\n",
    "        'chuva fraca': 1,\n",
    "        'chuva moderada': 2,\n",
    "        'chuva forte': 3,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def decision_tree_data_preparation(df: DataFrame) -> DataFrame:\n",
    "    prep_df = df.copy()\n",
    "\n",
    "    dropped_columns = ['city_name', 'avg_precipitation', 'magnitude_of_delay']\n",
    "\n",
    "    numerical_features = [column for column, dtype in zip(features.columns, features.dtypes) if\n",
    "                          dtype.kind in ['i', 'f'] and column not in dropped_columns]\n",
    "\n",
    "    numerical_features = ['avg_temperature', 'avg_atm_pressure', 'avg_humidity', 'avg_wind_speed', 'luminosity']\n",
    "    #assert numerical_features == ['avg_temperature', 'avg_atm_pressure', 'avg_humidity', 'avg_wind_speed', 'luminosity']\n",
    "\n",
    "    categorical_features = [column for column, dtype in zip(features.columns, features.dtypes) if\n",
    "                            dtype.kind not in ['i', 'f'] and column not in dropped_columns]\n",
    "\n",
    "    prep_df = prep_df.drop(dropped_columns, axis=1)\n",
    "    prep_df.drop_duplicates()\n",
    "\n",
    "    ### Converter as features categoricas em numericas\n",
    "    prep_df.replace(categorical_to_numerical, inplace=True)\n",
    "\n",
    "    ### Extrair a hora e dia da semana da feature 'record_date'\n",
    "    record_date = pd.DatetimeIndex(prep_df['record_date'])\n",
    "\n",
    "    prep_df['hour'] = record_date.hour\n",
    "    prep_df['day'] = record_date.day\n",
    "    prep_df['month'] = record_date.month\n",
    "    prep_df['weekday'] = record_date.weekday\n",
    "\n",
    "    prep_df.drop(columns=['record_date'], inplace=True)\n",
    "\n",
    "    #train_df['affected_roads'] = train_df['affected_roads'].fillna(train_df['affected_roads'].mode().iloc[0])\n",
    "\n",
    "    num_affected_roads = []\n",
    "    for line in prep_df['affected_roads']:\n",
    "        unique_roads = set(str(line).split(','))\n",
    "        valid_roads = [elem for elem in unique_roads if elem != '']\n",
    "        count = len(valid_roads)\n",
    "        num_affected_roads.append(count)\n",
    "\n",
    "    prep_df['num_affected_roads'] = num_affected_roads\n",
    "\n",
    "    ### Ao analisar o resultado pós-tratamento, verificámos que a feature 'affected_roads' tinha alta correlação com 'delay_in_minutes'\n",
    "    prep_df.drop(columns=['affected_roads'], inplace=True)\n",
    "\n",
    "    ### Converter a feature 'delay_in_seconds' para 'delay_in_minutes' de modo a reduzir o intervalo de valores\n",
    "    delay_in_minutes = prep_df['delay_in_seconds'].map(lambda seconds: seconds / 60)\n",
    "\n",
    "    prep_df.drop(columns=['delay_in_seconds'], inplace=True)\n",
    "    prep_df['delay_in_minutes'] = delay_in_minutes\n",
    "\n",
    "    ### Limites superior e inferior (sem outliers) dos diagramas de caixa\n",
    "    #numerical_features.remove('delay_in_seconds')\n",
    "    #numerical_features.append('delay_in_minutes')\n",
    "\n",
    "    return prep_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Divisão dos dados em dados de teste e treino"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "       incidents\n0              0\n1              0\n2              0\n3              0\n4              0\n...          ...\n10135          2\n10136          2\n10137          2\n10138          2\n10139          2\n\n[10140 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>incidents</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>10135</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10136</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10137</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10138</th>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>10139</th>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n<p>10140 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_num = pd.DataFrame(target.map({'None': 0, 'Low': 1, 'Medium': 2, 'High': 3, 'Very_High': 4}))\n",
    "target_num.reset_index(drop=True, inplace=True)\n",
    "target_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(decision_tree_data_preparation(features), target_num, test_size=0.3,\n",
    "                                                    random_state=2000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      incidents\n",
      "8721          2\n",
      "5465          1\n",
      "2527          3\n",
      "5321          1\n",
      "3987          3\n",
      "...         ...\n",
      "9628          2\n",
      "4380          1\n",
      "1590          0\n",
      "4045          3\n",
      "4936          1\n",
      "\n",
      "[7098 rows x 1 columns]\n",
      "      incidents\n",
      "1564          0\n",
      "9052          2\n",
      "8684          2\n",
      "7491          4\n",
      "389           0\n",
      "...         ...\n",
      "1288          0\n",
      "4606          1\n",
      "4999          1\n",
      "6644          4\n",
      "2501          3\n",
      "\n",
      "[3042 rows x 1 columns]\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb\n",
    "\n",
    "print(y_train)\n",
    "print(y_test)\n",
    "#boost_model = XGBClassifier(n_estimators=500, early_stopping_rounds=5, random_state=22)\n",
    "#boost_model.fit(X_train, y_train, eval_set=[(X_test, y_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#print(\"Train accuracy:\", boost_model.score(X_train, y_train))\n",
    "\n",
    "#predictions = boost_model.predict(X_test)\n",
    "#print(\"Test accuracy: \" + str(accuracy_score(predictions, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "Fitting 2 folds for each of 5 candidates, totalling 10 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gonca\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\sklearn.py:1224: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
      "C:\\Users\\gonca\\AppData\\Roaming\\Python\\Python39\\site-packages\\xgboost\\data.py:250: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23:06:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.5.0/src/learner.cc:1115: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'None'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[26], line 37\u001B[0m\n\u001B[0;32m     34\u001B[0m clf\u001B[38;5;241m.\u001B[39mfit(X_train, y_train)\n\u001B[0;32m     36\u001B[0m estimators\u001B[38;5;241m.\u001B[39mappend(clf\u001B[38;5;241m.\u001B[39mbest_estimator_)\n\u001B[1;32m---> 37\u001B[0m \u001B[43mresults\u001B[49m\u001B[43m[\u001B[49m\u001B[43mtest_index\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;241m=\u001B[39m clf\u001B[38;5;241m.\u001B[39mpredict(X_test)\n\u001B[0;32m     38\u001B[0m score \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m accuracy_score(results[test_index], y_test)\n\u001B[0;32m     39\u001B[0m lista_resultados\u001B[38;5;241m.\u001B[39mappend(accuracy_score(results[test_index], y_test))\n",
      "\u001B[1;31mValueError\u001B[0m: could not convert string to float: 'None'"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV, KFold\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "X = decision_tree_data_preparation(features)\n",
    "y = pd.DataFrame(target)\n",
    "print(type(y_test))\n",
    "#y = pd.concat([y_test, y_train])\n",
    "clf_xgb = XGBClassifier(objective='binary:logistic')\n",
    "\n",
    "param_dist = {\n",
    "    'n_estimators': stats.randint(150, 500),\n",
    "    'learning_rate': stats.uniform(0.01, 0.07),\n",
    "    'subsample': stats.uniform(0.3, 0.7),\n",
    "    'max_depth': [3, 5, 8],\n",
    "    'colsample_bytree': stats.uniform(0.5, 0.45),\n",
    "    'min_child_weight': [1, 3]\n",
    "}\n",
    "\n",
    "clf = RandomizedSearchCV(clf_xgb, param_distributions=param_dist, verbose=3,\n",
    "                         cv=2, n_iter=5, scoring='accuracy', error_score=0,\n",
    "                         n_jobs=-1)\n",
    "numFolds = 2\n",
    "folds = KFold(n_splits=numFolds, shuffle=True)\n",
    "\n",
    "estimators = []\n",
    "results = np.zeros(len(X))\n",
    "score = 0.0\n",
    "lista_resultados = []\n",
    "for train_index, test_index in folds.split(X):\n",
    "    X_train, X_test = X.iloc[train_index, :], X.iloc[test_index, :]\n",
    "    y_train, y_test = y.iloc[train_index].values.ravel(), y.iloc[test_index].values.ravel()\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    estimators.append(clf.best_estimator_)\n",
    "    results[test_index] = clf.predict(X_test)\n",
    "    score += accuracy_score(results[test_index], y_test)\n",
    "    lista_resultados.append(accuracy_score(results[test_index], y_test))\n",
    "    #score += f1_score(y_test, results[test_index])\n",
    "score /= numFolds\n",
    "\n",
    "print(f\"Minimo é: {min(lista_resultados)}\")\n",
    "'''\n",
    "def hyper_param(model, params):\n",
    "    print(\"Modelo atual: \" , model)\n",
    "    \n",
    "    # cv = cross-validation generator\n",
    "    # verbose = quanto é apresentado\n",
    "    tuning_model=GridSearchCV(model,param_grid=params,scoring='neg_mean_squared_error',cv=3)\n",
    "    tuning_model.fit(X_train, y_train)\n",
    "    \n",
    "    return tuning_model.best_params_\n",
    "\n",
    "params = {\n",
    "    # Parameters that we are going to tune.\n",
    "    'max_depth':6,\n",
    "    'min_child_weight': 1,\n",
    "    'eta':.3,\n",
    "    'subsample': 1,\n",
    "    'colsample_bytree': 1,\n",
    "    # Other parameters\n",
    "    'objective':'reg:squarederror',\n",
    "}\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "cv_results = xgb.cv(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=999,\n",
    "    seed=42,\n",
    "    nfold=5,\n",
    "    metrics = 'error',\n",
    "    early_stopping_rounds=10\n",
    ")\n",
    "print(cv_results)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtenção das previsões do dataset de submissão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = test_df.copy()\n",
    "\n",
    "test_data_prepared = decision_tree_data_preparation(test_data)\n",
    "\n",
    "predictions = boost_model.predict(test_data_prepared)  #RF_Model.predict(test_data_prepared)\n",
    "predictions_df = pd.DataFrame(predictions)\n",
    "predictions_df = predictions_df[0].map(\n",
    "    {0: 'None', 1: 'Low', 2: 'Medium', 3: 'High', 4: 'Very_High'})\n",
    "predictions_df.index += 1\n",
    "predictions_df.to_csv(\"../submission_v2.csv\", header=['Incidents'], index_label='RowId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
